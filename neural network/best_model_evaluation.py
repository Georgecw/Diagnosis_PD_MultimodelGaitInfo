import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰å‡½æ•°
from nn_reduced import ImprovedNN, load_reduced_data, preprocess_data, train_model_with_early_stopping, evaluate_model

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def train_best_model():
    """ä½¿ç”¨è¶…å‚æ•°æœç´¢å‘ç°çš„æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"""
    
    # ä»è¶…å‚æ•°æœç´¢ä¸­å‘ç°çš„æœ€ä½³å‚æ•°
    best_params = {
        'hidden_size': 20,
        'learning_rate': 0.0005,
        'dropout_rate': 0.5,
        'batch_size': 8,
        'weight_decay': 0.0001,
        'max_epochs': 300,
        'patience': 50
    }
    
    print("ğŸ† ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒæ¨¡å‹...")
    print(f"æœ€ä½³å‚æ•°: {best_params}")
    
    # åŠ è½½æ•°æ®
    X_train, y_train, X_test, y_test = load_reduced_data('_f_classif_k15')
    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, label_encoder = preprocess_data(
        X_train, y_train, X_test, y_test
    )
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæœ€ä½³æ¨¡å‹
    input_size = X_train_tensor.shape[1]
    output_size = len(label_encoder.classes_)
    
    model = ImprovedNN(
        input_size=input_size,
        hidden_size=best_params['hidden_size'],
        output_size=output_size,
        dropout_rate=best_params['dropout_rate']
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=best_params['batch_size'], 
        shuffle=True
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=best_params['learning_rate'],
        weight_decay=best_params['weight_decay']
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒæœ€ä½³æ¨¡å‹...")
    train_losses, train_accuracies, val_losses, val_accuracies = train_model_with_early_stopping(
        model, train_loader, criterion, optimizer, 
        X_test_tensor, y_test_tensor,
        num_epochs=best_params['max_epochs'],
        patience=best_params['patience']
    )
    
    # è¯„ä¼°æ¨¡å‹
    test_accuracy, test_f1, y_test_labels, predicted_labels = evaluate_model(
        model, X_test_tensor, y_test_tensor, label_encoder
    )
    
    return (model, test_accuracy, test_f1, y_test_labels, predicted_labels, 
            label_encoder, train_losses, train_accuracies, val_losses, val_accuracies, best_params)

def create_comprehensive_evaluation(y_test_labels, predicted_labels, label_encoder, 
                                  train_losses, train_accuracies, val_losses, val_accuracies, 
                                  best_params, test_accuracy, test_f1):
    """åˆ›å»ºç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
    
    # åˆ›å»ºå¤§å›¾
    fig = plt.figure(figsize=(20, 12))
    
    # 1. æ··æ·†çŸ©é˜µ (å æ®å·¦ä¾§å¤§éƒ¨åˆ†ç©ºé—´)
    ax1 = plt.subplot2grid((3, 4), (0, 0), colspan=2, rowspan=2)
    cm = confusion_matrix(y_test_labels, predicted_labels)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_encoder.classes_, 
                yticklabels=label_encoder.classes_,
                ax=ax1, cbar_kws={'shrink': 0.8}, square=True, linewidths=0.5)
    ax1.set_title(f'ğŸ¯ æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ\næµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.1%}, F1åˆ†æ•°: {test_f1:.1%}', 
                  fontsize=16, fontweight='bold', pad=20)
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ ç™¾åˆ†æ¯”
    total = np.sum(cm)
    for i in range(len(label_encoder.classes_)):
        for j in range(len(label_encoder.classes_)):
            percentage = cm[i, j] / total * 100
            ax1.text(j + 0.7, i + 0.3, f'({percentage:.1f}%)', 
                    ha='center', va='center', fontsize=10, color='gray')
    
    # 2. è®­ç»ƒå†å² - æŸå¤±
    ax2 = plt.subplot2grid((3, 4), (0, 2), colspan=1)
    epochs = range(1, len(train_losses) + 1)
    ax2.plot(epochs, train_losses, 'b-', label='è®­ç»ƒ', linewidth=2, alpha=0.8)
    ax2.plot(epochs, val_losses, 'r-', label='éªŒè¯', linewidth=2, alpha=0.8)
    ax2.set_title('ğŸ“‰ æŸå¤±æ›²çº¿', fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. è®­ç»ƒå†å² - å‡†ç¡®ç‡
    ax3 = plt.subplot2grid((3, 4), (0, 3), colspan=1)
    ax3.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒ', linewidth=2, alpha=0.8)
    ax3.plot(epochs, val_accuracies, 'r-', label='éªŒè¯', linewidth=2, alpha=0.8)
    ax3.set_title('ğŸ“ˆ å‡†ç¡®ç‡æ›²çº¿', fontweight='bold')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Accuracy (%)')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. è¿‡æ‹Ÿåˆåˆ†æ
    ax4 = plt.subplot2grid((3, 4), (1, 2), colspan=1)
    overfitting = [train_acc - val_acc for train_acc, val_acc in zip(train_accuracies, val_accuracies)]
    ax4.plot(epochs, overfitting, 'g-', linewidth=2, alpha=0.8)
    ax4.set_title('ğŸ” è¿‡æ‹Ÿåˆç¨‹åº¦', fontweight='bold')
    ax4.set_xlabel('Epoch')
    ax4.set_ylabel('è®­ç»ƒ-éªŒè¯å·®å€¼ (%)')
    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)
    ax4.grid(True, alpha=0.3)
    
    # 5. å„ç±»åˆ«æ€§èƒ½æŸ±çŠ¶å›¾
    ax5 = plt.subplot2grid((3, 4), (1, 3), colspan=1)
    report = classification_report(y_test_labels, predicted_labels, output_dict=True)
    classes = label_encoder.classes_
    f1_scores = [report[cls]['f1-score'] if cls in report else 0 for cls in classes]
    
    bars = ax5.bar(classes, f1_scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
    ax5.set_title('ğŸ“Š å„ç±»åˆ«F1åˆ†æ•°', fontweight='bold')
    ax5.set_ylabel('F1 Score')
    ax5.set_ylim(0, 1)
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for bar, score in zip(bars, f1_scores):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 6. æœ€ä½³å‚æ•°ä¿¡æ¯
    ax6 = plt.subplot2grid((3, 4), (2, 0), colspan=4)
    ax6.axis('off')
    
    # åˆ›å»ºå‚æ•°ä¿¡æ¯è¡¨æ ¼
    param_info = [
        ['å‚æ•°', 'æ•°å€¼', 'è¯´æ˜'],
        ['éšè—å±‚å¤§å°', f"{best_params['hidden_size']}", 'ç¥ç»ç½‘ç»œéšè—å±‚ç¥ç»å…ƒæ•°é‡'],
        ['å­¦ä¹ ç‡', f"{best_params['learning_rate']}", 'æ¨¡å‹å‚æ•°æ›´æ–°æ­¥é•¿'],
        ['Dropoutç‡', f"{best_params['dropout_rate']}", 'æ­£åˆ™åŒ–å¼ºåº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ'],
        ['æ‰¹å¤§å°', f"{best_params['batch_size']}", 'æ¯æ¬¡è®­ç»ƒçš„æ ·æœ¬æ•°é‡'],
        ['æƒé‡è¡°å‡', f"{best_params['weight_decay']}", 'L2æ­£åˆ™åŒ–ç³»æ•°'],
        ['è®­ç»ƒè½®æ•°', f"{len(train_losses)}", 'å®é™…è®­ç»ƒçš„epochæ•°é‡'],
        ['æœ€ç»ˆè¿‡æ‹Ÿåˆç¨‹åº¦', f"{overfitting[-1]:.2f}%", 'è®­ç»ƒä¸éªŒè¯å‡†ç¡®ç‡å·®å€¼']
    ]
    
    # åˆ›å»ºè¡¨æ ¼
    table = ax6.table(cellText=param_info[1:], colLabels=param_info[0],
                     cellLoc='center', loc='center',
                     colWidths=[0.2, 0.15, 0.4])
    table.auto_set_font_size(False)
    table.set_fontsize(11)
    table.scale(1, 2)
    
    # è®¾ç½®è¡¨æ ¼æ ·å¼
    for i in range(len(param_info)):
        for j in range(3):
            if i == 0:  # æ ‡é¢˜è¡Œ
                table[(i, j)].set_facecolor('#4CAF50')
                table[(i, j)].set_text_props(weight='bold', color='white')
            else:
                if j == 1:  # æ•°å€¼åˆ—
                    table[(i, j)].set_facecolor('#E3F2FD')
                    table[(i, j)].set_text_props(weight='bold')
    
    ax6.set_title('ğŸ›ï¸ æœ€ä½³è¶…å‚æ•°é…ç½® & è®­ç»ƒç»“æœ', fontsize=14, fontweight='bold', pad=20)
    
    plt.suptitle('ğŸ† å¸•é‡‘æ£®ç—…æ­¥æ€åˆ†æ - æœ€ä½³ç¥ç»ç½‘ç»œæ¨¡å‹å®Œæ•´è¯„ä¼°æŠ¥å‘Š', 
                 fontsize=18, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.subplots_adjust(top=0.94)
    plt.savefig('neural network/results/best_model_complete_evaluation.png', dpi=300, bbox_inches='tight')
    plt.show()

def create_detailed_confusion_matrix(y_test_labels, predicted_labels, label_encoder, test_accuracy, test_f1):
    """åˆ›å»ºè¯¦ç»†çš„æ··æ·†çŸ©é˜µå•ç‹¬å›¾"""
    
    cm = confusion_matrix(y_test_labels, predicted_labels)
    
    # è®¡ç®—å„ç§æŒ‡æ ‡
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # 1. ç»å¯¹æ•°é‡æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=label_encoder.classes_, 
                yticklabels=label_encoder.classes_,
                ax=ax1, square=True, linewidths=1)
    ax1.set_title(f'æ··æ·†çŸ©é˜µ - ç»å¯¹æ•°é‡\næ€»ä½“å‡†ç¡®ç‡: {test_accuracy:.1%}', 
                  fontsize=14, fontweight='bold')
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    # 2. ç™¾åˆ†æ¯”æ··æ·†çŸ©é˜µ
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges', 
                xticklabels=label_encoder.classes_, 
                yticklabels=label_encoder.classes_,
                ax=ax2, square=True, linewidths=1)
    ax2.set_title(f'æ··æ·†çŸ©é˜µ - æŒ‰è¡Œç™¾åˆ†æ¯”\nF1åˆ†æ•°: {test_f1:.1%}', 
                  fontsize=14, fontweight='bold')
    ax2.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    ax2.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    
    plt.suptitle('ğŸ¯ æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µè¯¦ç»†åˆ†æ', fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig('neural network/results/detailed_confusion_matrix.png', dpi=300, bbox_inches='tight')
    plt.show()

def print_detailed_results(y_test_labels, predicted_labels, label_encoder, test_accuracy, test_f1):
    """æ‰“å°è¯¦ç»†çš„ç»“æœåˆ†æ"""
    
    print("\n" + "="*60)
    print("ğŸ† æœ€ä½³æ¨¡å‹è¯¦ç»†è¯„ä¼°ç»“æœ")
    print("="*60)
    
    print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½:")
    print(f"   æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f} ({test_accuracy:.1%})")
    print(f"   æµ‹è¯•é›†F1åˆ†æ•°: {test_f1:.4f} ({test_f1:.1%})")
    
    print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    report = classification_report(y_test_labels, predicted_labels, output_dict=True)
    
    print(f"{'ç±»åˆ«':<12} {'ç²¾ç¡®ç‡':<8} {'å¬å›ç‡':<8} {'F1åˆ†æ•°':<8} {'æ”¯æŒæ•°':<6}")
    print("-" * 50)
    
    for class_name in label_encoder.classes_:
        if class_name in report:
            precision = report[class_name]['precision']
            recall = report[class_name]['recall']
            f1_score = report[class_name]['f1-score']
            support = int(report[class_name]['support'])
            print(f"{class_name:<12} {precision:<8.3f} {recall:<8.3f} {f1_score:<8.3f} {support:<6}")
        else:
            print(f"{class_name:<12} {'N/A':<8} {'N/A':<8} {'N/A':<8} {'0':<6}")
    
    print(f"\nğŸ” æ··æ·†çŸ©é˜µåˆ†æ:")
    cm = confusion_matrix(y_test_labels, predicted_labels)
    total_samples = np.sum(cm)
    
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   æ­£ç¡®é¢„æµ‹: {np.trace(cm)} ({np.trace(cm)/total_samples:.1%})")
    print(f"   é”™è¯¯é¢„æµ‹: {total_samples - np.trace(cm)} ({(total_samples - np.trace(cm))/total_samples:.1%})")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¯„ä¼°æœ€ä½³è¶…å‚æ•°æ¨¡å‹...")
    
    # è®­ç»ƒæœ€ä½³æ¨¡å‹
    (model, test_accuracy, test_f1, y_test_labels, predicted_labels, 
     label_encoder, train_losses, train_accuracies, val_losses, val_accuracies, best_params) = train_best_model()
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print_detailed_results(y_test_labels, predicted_labels, label_encoder, test_accuracy, test_f1)
    
    # åˆ›å»ºç»¼åˆè¯„ä¼°æŠ¥å‘Š
    print("\nğŸ“Š ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
    create_comprehensive_evaluation(
        y_test_labels, predicted_labels, label_encoder,
        train_losses, train_accuracies, val_losses, val_accuracies,
        best_params, test_accuracy, test_f1
    )
    
    # åˆ›å»ºè¯¦ç»†æ··æ·†çŸ©é˜µ
    print("\nğŸ¯ ç”Ÿæˆè¯¦ç»†æ··æ·†çŸ©é˜µ...")
    create_detailed_confusion_matrix(y_test_labels, predicted_labels, label_encoder, test_accuracy, test_f1)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹...")
    torch.save(model.state_dict(), 'neural network/results/final_best_model.pth')
    
    print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- neural network/results/best_model_complete_evaluation.png")
    print("- neural network/results/detailed_confusion_matrix.png")
    print("- neural network/results/final_best_model.pth")
    
    return model, test_accuracy, test_f1, y_test_labels, predicted_labels

if __name__ == "__main__":
    results = main()

