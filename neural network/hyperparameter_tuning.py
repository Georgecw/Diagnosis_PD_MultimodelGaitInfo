import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥è‡ªå®šä¹‰å‡½æ•°
from nn_reduced import ImprovedNN, load_reduced_data, preprocess_data, train_model_with_early_stopping, evaluate_model

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

class HyperparameterTuner:
    """è¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self, data_suffix='_f_classif_k15'):
        """
        åˆå§‹åŒ–è¶…å‚æ•°è°ƒä¼˜å™¨
        
        Parameters:
        -----------
        data_suffix : str
            æ•°æ®æ–‡ä»¶åç¼€ï¼Œé»˜è®¤ä½¿ç”¨k=15çš„æ•°æ®
        """
        self.data_suffix = data_suffix
        self.results = []
        self.best_params = None
        self.best_score = 0
        
    def load_data(self):
        """åŠ è½½æ•°æ®"""
        print(f"åŠ è½½æ•°æ®: {self.data_suffix}")
        X_train, y_train, X_test, y_test = load_reduced_data(self.data_suffix)
        
        if X_train is None:
            raise ValueError(f"æ— æ³•åŠ è½½æ•°æ®æ–‡ä»¶: {self.data_suffix}")
        
        # é¢„å¤„ç†æ•°æ®
        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, label_encoder = preprocess_data(
            X_train, y_train, X_test, y_test
        )
        
        self.X_train = X_train_tensor
        self.y_train = y_train_tensor
        self.X_test = X_test_tensor
        self.y_test = y_test_tensor
        self.label_encoder = label_encoder
        self.input_size = X_train_tensor.shape[1]
        self.output_size = len(label_encoder.classes_)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: è¾“å…¥ç»´åº¦={self.input_size}, è¾“å‡ºç»´åº¦={self.output_size}")
        
    def grid_search(self, param_grid, max_trials=None):
        """
        ç½‘æ ¼æœç´¢è¶…å‚æ•°
        
        Parameters:
        -----------
        param_grid : dict
            å‚æ•°ç½‘æ ¼ï¼ŒåŒ…å«è¦æœç´¢çš„è¶…å‚æ•°èŒƒå›´
        max_trials : int, optional
            æœ€å¤§è¯•éªŒæ¬¡æ•°ï¼ŒNoneè¡¨ç¤ºæœç´¢æ‰€æœ‰ç»„åˆ
        """
        print("å¼€å§‹ç½‘æ ¼æœç´¢...")
        print(f"å‚æ•°ç½‘æ ¼: {param_grid}")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        param_combinations = list(product(*param_values))
        
        if max_trials and len(param_combinations) > max_trials:
            # éšæœºé€‰æ‹©éƒ¨åˆ†ç»„åˆ
            np.random.shuffle(param_combinations)
            param_combinations = param_combinations[:max_trials]
        
        print(f"æ€»å…±è¦æµ‹è¯• {len(param_combinations)} ç§å‚æ•°ç»„åˆ")
        
        for i, param_combo in enumerate(param_combinations):
            print(f"\n--- è¯•éªŒ {i+1}/{len(param_combinations)} ---")
            
            # åˆ›å»ºå‚æ•°å­—å…¸
            params = dict(zip(param_names, param_combo))
            print(f"å½“å‰å‚æ•°: {params}")
            
            try:
                # è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
                result = self._train_and_evaluate(params)
                result['trial'] = i + 1
                result['params'] = params.copy()
                self.results.append(result)
                
                # æ›´æ–°æœ€ä½³å‚æ•°
                if result['test_accuracy'] > self.best_score:
                    self.best_score = result['test_accuracy']
                    self.best_params = params.copy()
                    print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä½³å‚æ•°ï¼å‡†ç¡®ç‡: {self.best_score:.4f}")
                
            except Exception as e:
                print(f"âŒ è¯•éªŒå¤±è´¥: {e}")
                continue
        
        print(f"\nğŸ† æœç´¢å®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {self.best_score:.4f}")
        print(f"ğŸ† æœ€ä½³å‚æ•°: {self.best_params}")
        
    def _train_and_evaluate(self, params):
        """è®­ç»ƒå’Œè¯„ä¼°å•ä¸ªå‚æ•°ç»„åˆ"""
        # è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡ç°æ€§
        torch.manual_seed(42)
        np.random.seed(42)
        
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedNN(
            input_size=self.input_size,
            hidden_size=params['hidden_size'],
            output_size=self.output_size,
            dropout_rate=params['dropout_rate']
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(self.X_train, self.y_train)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=params['batch_size'], 
            shuffle=True
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(
            model.parameters(), 
            lr=params['learning_rate'],
            weight_decay=params['weight_decay']
        )
        
        # è®­ç»ƒæ¨¡å‹
        train_losses, train_accuracies, val_losses, val_accuracies = train_model_with_early_stopping(
            model, train_loader, criterion, optimizer, 
            self.X_test, self.y_test,
            num_epochs=params['max_epochs'],
            patience=params['patience']
        )
        
        # è¯„ä¼°æ¨¡å‹
        test_accuracy, test_f1, _, _ = evaluate_model(
            model, self.X_test, self.y_test, self.label_encoder
        )
        
        # è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦
        final_train_acc = train_accuracies[-1] if train_accuracies else 0
        final_val_acc = val_accuracies[-1] if val_accuracies else 0
        overfitting = final_train_acc - final_val_acc
        
        result = {
            'test_accuracy': test_accuracy,
            'test_f1': test_f1,
            'train_accuracy': final_train_acc,
            'val_accuracy': final_val_acc,
            'overfitting': overfitting,
            'epochs_trained': len(train_losses),
            'final_train_loss': train_losses[-1] if train_losses else float('inf'),
            'final_val_loss': val_losses[-1] if val_losses else float('inf')
        }
        
        print(f"ç»“æœ: æµ‹è¯•å‡†ç¡®ç‡={test_accuracy:.4f}, F1={test_f1:.4f}, è¿‡æ‹Ÿåˆç¨‹åº¦={overfitting:.2f}%")
        
        return result
    
    def analyze_results(self):
        """åˆ†æè¶…å‚æ•°è°ƒä¼˜ç»“æœ"""
        if not self.results:
            print("æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(self.results)
        
        # å±•å¼€å‚æ•°åˆ—
        params_df = pd.json_normalize(df['params'])
        df = pd.concat([df.drop('params', axis=1), params_df], axis=1)
        
        print("\n=== è¶…å‚æ•°è°ƒä¼˜ç»“æœåˆ†æ ===")
        
        # 1. æœ€ä½³ç»“æœ
        best_result = df.loc[df['test_accuracy'].idxmax()]
        print(f"\nğŸ† æœ€ä½³ç»“æœ:")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {best_result['test_accuracy']:.4f}")
        print(f"  æµ‹è¯•F1åˆ†æ•°: {best_result['test_f1']:.4f}")
        print(f"  éšè—å±‚å¤§å°: {best_result['hidden_size']}")
        print(f"  å­¦ä¹ ç‡: {best_result['learning_rate']}")
        print(f"  Dropoutç‡: {best_result['dropout_rate']}")
        print(f"  æ‰¹å¤§å°: {best_result['batch_size']}")
        print(f"  æƒé‡è¡°å‡: {best_result['weight_decay']}")
        print(f"  è®­ç»ƒè½®æ•°: {best_result['epochs_trained']}")
        print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {best_result['overfitting']:.2f}%")
        
        # 2. å‚æ•°é‡è¦æ€§åˆ†æ
        print(f"\nğŸ“Š å‚æ•°å¯¹æ€§èƒ½çš„å½±å“:")
        for param in ['hidden_size', 'learning_rate', 'dropout_rate', 'batch_size', 'weight_decay']:
            if param in df.columns:
                correlation = df['test_accuracy'].corr(df[param])
                print(f"  {param}: ç›¸å…³ç³»æ•° = {correlation:.3f}")
        
        # 3. ç»Ÿè®¡æ‘˜è¦
        print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  å¹³å‡æµ‹è¯•å‡†ç¡®ç‡: {df['test_accuracy'].mean():.4f} Â± {df['test_accuracy'].std():.4f}")
        print(f"  æœ€é«˜æµ‹è¯•å‡†ç¡®ç‡: {df['test_accuracy'].max():.4f}")
        print(f"  æœ€ä½æµ‹è¯•å‡†ç¡®ç‡: {df['test_accuracy'].min():.4f}")
        print(f"  å¹³å‡è¿‡æ‹Ÿåˆç¨‹åº¦: {df['overfitting'].mean():.2f}% Â± {df['overfitting'].std():.2f}%")
        
        return df
    
    def plot_results(self, df=None):
        """å¯è§†åŒ–è¶…å‚æ•°è°ƒä¼˜ç»“æœ"""
        if df is None:
            df = self.analyze_results()
        
        if df is None or df.empty:
            return
        
        try:
            # åˆ›å»ºå­å›¾
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('è¶…å‚æ•°è°ƒä¼˜ç»“æœåˆ†æ', fontsize=16)
            
            # 1. éšè—å±‚å¤§å° vs å‡†ç¡®ç‡
            if 'hidden_size' in df.columns:
                axes[0,0].scatter(df['hidden_size'], df['test_accuracy'], alpha=0.7, c='blue')
                axes[0,0].set_xlabel('éšè—å±‚å¤§å°')
                axes[0,0].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,0].set_title('éšè—å±‚å¤§å° vs æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,0].grid(True, alpha=0.3)
            
            # 2. å­¦ä¹ ç‡ vs å‡†ç¡®ç‡
            if 'learning_rate' in df.columns:
                axes[0,1].scatter(df['learning_rate'], df['test_accuracy'], alpha=0.7, c='green')
                axes[0,1].set_xlabel('å­¦ä¹ ç‡')
                axes[0,1].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,1].set_title('å­¦ä¹ ç‡ vs æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,1].set_xscale('log')
                axes[0,1].grid(True, alpha=0.3)
            
            # 3. Dropoutç‡ vs å‡†ç¡®ç‡
            if 'dropout_rate' in df.columns:
                axes[0,2].scatter(df['dropout_rate'], df['test_accuracy'], alpha=0.7, c='red')
                axes[0,2].set_xlabel('Dropoutç‡')
                axes[0,2].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,2].set_title('Dropoutç‡ vs æµ‹è¯•å‡†ç¡®ç‡')
                axes[0,2].grid(True, alpha=0.3)
            
            # 4. è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†å¸ƒ
            axes[1,0].hist(df['overfitting'], bins=15, alpha=0.7, color='orange')
            axes[1,0].set_xlabel('è¿‡æ‹Ÿåˆç¨‹åº¦ (%)')
            axes[1,0].set_ylabel('é¢‘æ¬¡')
            axes[1,0].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦åˆ†å¸ƒ')
            axes[1,0].axvline(x=0, color='black', linestyle='--', alpha=0.5)
            axes[1,0].grid(True, alpha=0.3)
            
            # 5. è®­ç»ƒè½®æ•° vs å‡†ç¡®ç‡
            axes[1,1].scatter(df['epochs_trained'], df['test_accuracy'], alpha=0.7, c='purple')
            axes[1,1].set_xlabel('è®­ç»ƒè½®æ•°')
            axes[1,1].set_ylabel('æµ‹è¯•å‡†ç¡®ç‡')
            axes[1,1].set_title('è®­ç»ƒè½®æ•° vs æµ‹è¯•å‡†ç¡®ç‡')
            axes[1,1].grid(True, alpha=0.3)
            
            # 6. å‡†ç¡®ç‡æ’å
            df_sorted = df.sort_values('test_accuracy', ascending=False).head(10)
            axes[1,2].barh(range(len(df_sorted)), df_sorted['test_accuracy'])
            axes[1,2].set_xlabel('æµ‹è¯•å‡†ç¡®ç‡')
            axes[1,2].set_ylabel('è¯•éªŒæ’å')
            axes[1,2].set_title('Top 10 è¯•éªŒç»“æœ')
            axes[1,2].set_yticks(range(len(df_sorted)))
            axes[1,2].set_yticklabels([f"Trial {int(x)}" for x in df_sorted['trial']])
            axes[1,2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('neural network/results/hyperparameter_tuning_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
        except Exception as e:
            print(f"ç»˜å›¾æ—¶å‡ºç°é”™è¯¯: {e}")
            print("è·³è¿‡å¯è§†åŒ–æ­¥éª¤")
    
    def save_results(self, filename='hyperparameter_results.csv'):
        """ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶"""
        if not self.results:
            print("æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return
        
        df = pd.DataFrame(self.results)
        params_df = pd.json_normalize(df['params'])
        df = pd.concat([df.drop('params', axis=1), params_df], axis=1)
        
        filepath = f'neural network/results/{filename}'
        df.to_csv(filepath, index=False)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {filepath}")

def quick_search():
    """å¿«é€Ÿæœç´¢ - ç²—ç²’åº¦ç½‘æ ¼"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿè¶…å‚æ•°æœç´¢...")
    
    tuner = HyperparameterTuner('_f_classif_k15')  # ä½¿ç”¨æœ€ä½³ç‰¹å¾æ•°é‡
    tuner.load_data()
    
    # å®šä¹‰æœç´¢ç©ºé—´ - è¾ƒç²—çš„ç½‘æ ¼
    param_grid = {
        'hidden_size': [8, 12, 16, 20, 24, 32],  # ä¸åŒçš„éšè—å±‚å¤§å°
        'learning_rate': [0.0005, 0.001, 0.002, 0.005],  # ä¸åŒå­¦ä¹ ç‡
        'dropout_rate': [0.2, 0.3, 0.4, 0.5],  # ä¸åŒdropoutç‡
        'batch_size': [4, 8, 16],  # ä¸åŒæ‰¹å¤§å°
        'weight_decay': [1e-5, 1e-4, 1e-3],  # ä¸åŒæƒé‡è¡°å‡
        'max_epochs': [200],  # å›ºå®šæœ€å¤§è½®æ•°
        'patience': [30]  # å›ºå®šè€å¿ƒå€¼
    }
    
    tuner.grid_search(param_grid, max_trials=50)  # é™åˆ¶æœ€å¤§è¯•éªŒæ¬¡æ•°
    df = tuner.analyze_results()
    tuner.plot_results(df)
    tuner.save_results('quick_search_results.csv')
    
    return tuner

def fine_search(best_params=None):
    """ç²¾ç»†æœç´¢ - åœ¨æœ€ä½³å‚æ•°é™„è¿‘æœç´¢"""
    print("ğŸ¯ å¼€å§‹ç²¾ç»†è¶…å‚æ•°æœç´¢...")
    
    tuner = HyperparameterTuner('_f_classif_k15')
    tuner.load_data()
    
    if best_params is None:
        # ä½¿ç”¨é»˜è®¤çš„è¾ƒå¥½å‚æ•°ä½œä¸ºä¸­å¿ƒ
        best_params = {
            'hidden_size': 16,
            'learning_rate': 0.001,
            'dropout_rate': 0.4,
            'batch_size': 8,
            'weight_decay': 1e-4
        }
    
    # åœ¨æœ€ä½³å‚æ•°é™„è¿‘å®šä¹‰ç²¾ç»†ç½‘æ ¼
    param_grid = {
        'hidden_size': [best_params['hidden_size'] - 4, best_params['hidden_size'], 
                       best_params['hidden_size'] + 4, best_params['hidden_size'] + 8],
        'learning_rate': [best_params['learning_rate'] * 0.5, best_params['learning_rate'],
                         best_params['learning_rate'] * 1.5, best_params['learning_rate'] * 2],
        'dropout_rate': [max(0.1, best_params['dropout_rate'] - 0.1), best_params['dropout_rate'],
                        min(0.6, best_params['dropout_rate'] + 0.1)],
        'batch_size': [max(4, best_params['batch_size'] // 2), best_params['batch_size'],
                      min(16, best_params['batch_size'] * 2)],
        'weight_decay': [best_params['weight_decay'] * 0.1, best_params['weight_decay'],
                        best_params['weight_decay'] * 10],
        'max_epochs': [300],
        'patience': [40]
    }
    
    tuner.grid_search(param_grid)
    df = tuner.analyze_results()
    tuner.plot_results(df)
    tuner.save_results('fine_search_results.csv')
    
    return tuner

def compare_feature_sizes():
    """æ¯”è¾ƒä¸åŒç‰¹å¾æ•°é‡ä¸‹çš„æœ€ä½³è¶…å‚æ•°"""
    print("ğŸ”„ æ¯”è¾ƒä¸åŒç‰¹å¾æ•°é‡çš„æœ€ä½³è¶…å‚æ•°...")
    
    feature_sizes = ['_f_classif_k12', '_f_classif_k15', '_f_classif_k18']
    all_results = {}
    
    for feature_size in feature_sizes:
        print(f"\n--- æµ‹è¯• {feature_size} ---")
        
        tuner = HyperparameterTuner(feature_size)
        tuner.load_data()
        
        # ä½¿ç”¨è¾ƒå°çš„æœç´¢ç©ºé—´
        param_grid = {
            'hidden_size': [8, 12, 16, 20, 24],
            'learning_rate': [0.001, 0.002],
            'dropout_rate': [0.3, 0.4, 0.5],
            'batch_size': [8],
            'weight_decay': [1e-4],
            'max_epochs': [200],
            'patience': [30]
        }
        
        tuner.grid_search(param_grid, max_trials=20)
        all_results[feature_size] = {
            'best_params': tuner.best_params,
            'best_score': tuner.best_score,
            'results': tuner.results
        }
    
    # æ¯”è¾ƒç»“æœ
    print("\n=== ä¸åŒç‰¹å¾æ•°é‡çš„æœ€ä½³ç»“æœæ¯”è¾ƒ ===")
    for feature_size, result in all_results.items():
        print(f"{feature_size}: å‡†ç¡®ç‡={result['best_score']:.4f}, æœ€ä½³å‚æ•°={result['best_params']}")
    
    return all_results

def train_and_evaluate_best_model(best_params, data_suffix='_f_classif_k15'):
    """ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹å¹¶ç”Ÿæˆè¯¦ç»†è¯„ä¼°ç»“æœ"""
    print(f"\nğŸ† ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    print(f"æœ€ä½³å‚æ•°: {best_params}")
    
    # åŠ è½½æ•°æ®
    X_train, y_train, X_test, y_test = load_reduced_data(data_suffix)
    X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, label_encoder = preprocess_data(
        X_train, y_train, X_test, y_test
    )
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    
    # åˆ›å»ºæœ€ä½³æ¨¡å‹
    input_size = X_train_tensor.shape[1]
    output_size = len(label_encoder.classes_)
    
    model = ImprovedNN(
        input_size=input_size,
        hidden_size=best_params['hidden_size'],
        output_size=output_size,
        dropout_rate=best_params['dropout_rate']
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=best_params['batch_size'], 
        shuffle=True
    )
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(
        model.parameters(), 
        lr=best_params['learning_rate'],
        weight_decay=best_params['weight_decay']
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒæœ€ä½³æ¨¡å‹...")
    train_losses, train_accuracies, val_losses, val_accuracies = train_model_with_early_stopping(
        model, train_loader, criterion, optimizer, 
        X_test_tensor, y_test_tensor,
        num_epochs=best_params.get('max_epochs', 300),
        patience=best_params.get('patience', 50)
    )
    
    # è¯„ä¼°æ¨¡å‹
    test_accuracy, test_f1, y_test_labels, predicted_labels = evaluate_model(
        model, X_test_tensor, y_test_tensor, label_encoder
    )
    
    return model, test_accuracy, test_f1, y_test_labels, predicted_labels, label_encoder, train_losses, train_accuracies, val_losses, val_accuracies

def plot_best_model_results(y_test_labels, predicted_labels, label_encoder, 
                           train_losses, train_accuracies, val_losses, val_accuracies, 
                           best_params, test_accuracy, test_f1):
    """ç»˜åˆ¶æœ€ä½³æ¨¡å‹çš„è¯¦ç»†ç»“æœ"""
    
    try:
        # åˆ›å»ºå­å›¾
        fig = plt.figure(figsize=(20, 12))
        
        # 1. æ··æ·†çŸ©é˜µ (å¤§å›¾)
        ax1 = plt.subplot(2, 3, (1, 4))  # å æ®å·¦ä¾§ä¸¤è¡Œ
        from sklearn.metrics import confusion_matrix
        cm = confusion_matrix(y_test_labels, predicted_labels)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=label_encoder.classes_, 
                    yticklabels=label_encoder.classes_,
                    ax=ax1, cbar_kws={'shrink': 0.8})
        ax1.set_title(f'æœ€ä½³æ¨¡å‹æ··æ·†çŸ©é˜µ\nå‡†ç¡®ç‡: {test_accuracy:.4f}, F1: {test_f1:.4f}', 
                      fontsize=14, fontweight='bold')
        ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
        ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
        
        # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ ç™¾åˆ†æ¯”
        total = np.sum(cm)
        for i in range(len(label_encoder.classes_)):
            for j in range(len(label_encoder.classes_)):
                percentage = cm[i, j] / total * 100
                ax1.text(j + 0.7, i + 0.3, f'({percentage:.1f}%)', 
                        ha='center', va='center', fontsize=10, color='gray')
        
        # 2. è®­ç»ƒå†å² - æŸå¤±
        ax2 = plt.subplot(2, 3, 2)
        epochs = range(1, len(train_losses) + 1)
        ax2.plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        ax2.plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        ax2.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±', fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. è®­ç»ƒå†å² - å‡†ç¡®ç‡
        ax3 = plt.subplot(2, 3, 3)
        ax3.plot(epochs, train_accuracies, 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        ax3.plot(epochs, val_accuracies, 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        ax3.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡', fontweight='bold')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Accuracy (%)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. è¿‡æ‹Ÿåˆåˆ†æ
        ax4 = plt.subplot(2, 3, 5)
        overfitting = [train_acc - val_acc for train_acc, val_acc in zip(train_accuracies, val_accuracies)]
        ax4.plot(epochs, overfitting, 'g-', linewidth=2)
        ax4.set_title('è¿‡æ‹Ÿåˆç¨‹åº¦ (è®­ç»ƒå‡†ç¡®ç‡ - éªŒè¯å‡†ç¡®ç‡)', fontweight='bold')
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Accuracy Difference (%)')
        ax4.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        ax4.grid(True, alpha=0.3)
        
        # 5. æœ€ä½³å‚æ•°å±•ç¤º
        ax5 = plt.subplot(2, 3, 6)
        ax5.axis('off')
        
        # å‚æ•°æ–‡æœ¬
        param_text = f"""æœ€ä½³è¶…å‚æ•°é…ç½®:
        
éšè—å±‚å¤§å°: {best_params['hidden_size']}
å­¦ä¹ ç‡: {best_params['learning_rate']}
Dropoutç‡: {best_params['dropout_rate']}
æ‰¹å¤§å°: {best_params['batch_size']}
æƒé‡è¡°å‡: {best_params['weight_decay']}

æœ€ç»ˆæ€§èƒ½:
æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}
æµ‹è¯•F1åˆ†æ•°: {test_f1:.4f}
è®­ç»ƒè½®æ•°: {len(train_losses)}
æœ€ç»ˆè¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting[-1]:.2f}%"""
        
        ax5.text(0.1, 0.9, param_text, transform=ax5.transAxes, fontsize=11,
                 verticalalignment='top', bbox=dict(boxstyle='round,pad=0.5', 
                 facecolor='lightblue', alpha=0.8))
        
        plt.suptitle('ğŸ† æœ€ä½³è¶…å‚æ•°æ¨¡å‹å®Œæ•´è¯„ä¼°æŠ¥å‘Š', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('neural network/results/best_model_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
    except Exception as e:
        print(f"ç»˜åˆ¶æœ€ä½³æ¨¡å‹ç»“æœæ—¶å‡ºç°é”™è¯¯: {e}")

def generate_classification_report_visualization(y_test_labels, predicted_labels, label_encoder):
    """ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šçš„å¯è§†åŒ–"""
    try:
        from sklearn.metrics import classification_report
        
        # è·å–åˆ†ç±»æŠ¥å‘Š
        report = classification_report(y_test_labels, predicted_labels, output_dict=True)
        
        # æå–å„ç±»åˆ«çš„æŒ‡æ ‡
        classes = label_encoder.classes_
        metrics = ['precision', 'recall', 'f1-score']
        
        # åˆ›å»ºæ•°æ®çŸ©é˜µ
        data = []
        for class_name in classes:
            if class_name in report:
                data.append([report[class_name][metric] for metric in metrics])
            else:
                data.append([0, 0, 0])  # å¦‚æœç±»åˆ«ä¸åœ¨æŠ¥å‘Šä¸­
        
        data = np.array(data)
        
        # ç»˜åˆ¶çƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(8, 6))
        im = ax.imshow(data, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(range(len(metrics)))
        ax.set_xticklabels(metrics)
        ax.set_yticks(range(len(classes)))
        ax.set_yticklabels(classes)
        
        # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ æ•°å€¼
        for i in range(len(classes)):
            for j in range(len(metrics)):
                text = ax.text(j, i, f'{data[i, j]:.3f}', 
                              ha='center', va='center', color='black', fontweight='bold')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, ax=ax, shrink=0.8)
        cbar.set_label('åˆ†æ•°', rotation=270, labelpad=15)
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title('å„ç±»åˆ«åˆ†ç±»æ€§èƒ½çƒ­åŠ›å›¾', fontsize=14, fontweight='bold', pad=20)
        
        plt.tight_layout()
        plt.savefig('neural network/results/classification_report_heatmap.png', dpi=300, bbox_inches='tight')
        plt.show()
    except Exception as e:
        print(f"ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šå¯è§†åŒ–æ—¶å‡ºç°é”™è¯¯: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ›ï¸ ç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒä¼˜ç¨‹åº")
    print("=" * 50)
    
    # 1. å¿«é€Ÿæœç´¢
    print("\n1ï¸âƒ£ æ‰§è¡Œå¿«é€Ÿæœç´¢...")
    quick_tuner = quick_search()
    
    # 2. åŸºäºå¿«é€Ÿæœç´¢ç»“æœè¿›è¡Œç²¾ç»†æœç´¢
    print("\n2ï¸âƒ£ åŸºäºå¿«é€Ÿæœç´¢ç»“æœæ‰§è¡Œç²¾ç»†æœç´¢...")
    fine_tuner = fine_search(quick_tuner.best_params)
    
    # 3. æ¯”è¾ƒä¸åŒç‰¹å¾æ•°é‡
    print("\n3ï¸âƒ£ æ¯”è¾ƒä¸åŒç‰¹å¾æ•°é‡çš„æœ€ä½³è¶…å‚æ•°...")
    comparison_results = compare_feature_sizes()
    
    # 4. ç¡®å®šå…¨å±€æœ€ä½³å‚æ•°
    print("\n4ï¸âƒ£ ç¡®å®šå…¨å±€æœ€ä½³å‚æ•°...")
    all_best_scores = [
        (quick_tuner.best_score, quick_tuner.best_params, "å¿«é€Ÿæœç´¢"),
        (fine_tuner.best_score, fine_tuner.best_params, "ç²¾ç»†æœç´¢")
    ]
    
    # æ·»åŠ ä¸åŒç‰¹å¾æ•°é‡çš„æœ€ä½³ç»“æœ
    for feature_size, result in comparison_results.items():
        all_best_scores.append((result['best_score'], result['best_params'], f"ç‰¹å¾æ•°é‡ {feature_size}"))
    
    # æ‰¾åˆ°å…¨å±€æœ€ä½³
    global_best_score, global_best_params, best_source = max(all_best_scores, key=lambda x: x[0])
    
    print(f"ğŸ† å…¨å±€æœ€ä½³ç»“æœæ¥è‡ª: {best_source}")
    print(f"ğŸ† å…¨å±€æœ€ä½³å‡†ç¡®ç‡: {global_best_score:.4f}")
    print(f"ğŸ† å…¨å±€æœ€ä½³å‚æ•°: {global_best_params}")
    
    # 5. ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹å¹¶ç”Ÿæˆè¯¦ç»†è¯„ä¼°
    print("\n5ï¸âƒ£ è®­ç»ƒæœ€ç»ˆæœ€ä½³æ¨¡å‹å¹¶ç”Ÿæˆè¯¦ç»†è¯„ä¼°...")
    (model, test_accuracy, test_f1, y_test_labels, predicted_labels, 
     label_encoder, train_losses, train_accuracies, val_losses, val_accuracies) = train_and_evaluate_best_model(
        global_best_params, '_f_classif_k15'
    )
    
    # 6. ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–ç»“æœ
    print("\n6ï¸âƒ£ ç”Ÿæˆæœ€ä½³æ¨¡å‹çš„è¯¦ç»†å¯è§†åŒ–ç»“æœ...")
    plot_best_model_results(
        y_test_labels, predicted_labels, label_encoder,
        train_losses, train_accuracies, val_losses, val_accuracies,
        global_best_params, test_accuracy, test_f1
    )
    
    # 7. ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾
    print("\n7ï¸âƒ£ ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šçƒ­åŠ›å›¾...")
    generate_classification_report_visualization(y_test_labels, predicted_labels, label_encoder)
    
    # 8. ä¿å­˜æœ€ä½³æ¨¡å‹
    print("\n8ï¸âƒ£ ä¿å­˜æœ€ä½³æ¨¡å‹...")
    torch.save(model.state_dict(), 'neural network/results/best_hyperparameter_model.pth')
    
    print("\nğŸ‰ è¶…å‚æ•°è°ƒä¼˜å®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    print("- neural network/results/hyperparameter_tuning_analysis.png")
    print("- neural network/results/quick_search_results.csv")
    print("- neural network/results/fine_search_results.csv")
    print("- neural network/results/best_model_evaluation.png (æ–°å¢)")
    print("- neural network/results/classification_report_heatmap.png (æ–°å¢)")
    print("- neural network/results/best_hyperparameter_model.pth (æ–°å¢)")
    
    return quick_tuner, fine_tuner, comparison_results, model, global_best_params

if __name__ == "__main__":
    results = main()
