{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee18b80",
   "metadata": {},
   "source": [
    "# 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b18a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 设置RawData文件夹路径\n",
    "raw_data_path = \"./RawData\"\n",
    "\n",
    "# 读取数据文件\n",
    "gait_para = pd.read_csv(os.path.join(raw_data_path, \"gait-parameters.csv\"))\n",
    "spec_stats = pd.read_csv(os.path.join(raw_data_path, \"spectral_stats.csv\"))\n",
    "tempo_stats = pd.read_csv(os.path.join(raw_data_path, \"temporal_stats.csv\"))\n",
    "PD_characteristics = pd.read_excel(\n",
    "    os.path.join(raw_data_path, \"PD_disease_characteristics.xlsx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf69c1b",
   "metadata": {},
   "source": [
    "# 合并有相同 ID 号的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09463d7c",
   "metadata": {},
   "source": [
    "## gait_para 步态参数数据\n",
    "\n",
    "对特征后缀的解释\n",
    "- ConOg 代表 condition=Overground\n",
    "- ConTm 代表 condition=Treadmill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696e586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离Overground和Treadmill数据\n",
    "gait_overground = gait_para[gait_para[\"condition\"] == \"Overground\"].copy()\n",
    "gait_treadmill = gait_para[gait_para[\"condition\"] == \"Treadmill\"].copy()\n",
    "\n",
    "# 删除condition列\n",
    "gait_overground = gait_overground.drop(\"condition\", axis=1)\n",
    "gait_treadmill = gait_treadmill.drop(\"condition\", axis=1)\n",
    "\n",
    "# 为除ID和Group外的列添加后缀\n",
    "columns_to_rename = [\n",
    "    col for col in gait_overground.columns if col not in [\"ID\", \"Group\"]\n",
    "]\n",
    "\n",
    "# 为Overground数据添加ConOg后缀\n",
    "overground_rename_dict = {col: f\"{col}_ConOg\" for col in columns_to_rename}\n",
    "gait_overground = gait_overground.rename(columns=overground_rename_dict)\n",
    "\n",
    "# 为Treadmill数据添加ConTm后缀\n",
    "treadmill_rename_dict = {col: f\"{col}_ConTm\" for col in columns_to_rename}\n",
    "gait_treadmill = gait_treadmill.rename(columns=treadmill_rename_dict)\n",
    "\n",
    "# 合并数据，基于ID进行外连接\n",
    "gait_para = pd.merge(\n",
    "    gait_overground,\n",
    "    gait_treadmill[\n",
    "        [\"ID\"] + [col for col in gait_treadmill.columns if col not in [\"ID\", \"Group\"]]\n",
    "    ],\n",
    "    on=\"ID\",\n",
    "    how=\"outer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb7431d",
   "metadata": {},
   "source": [
    "## spec_stats 频谱统计数据\n",
    "\n",
    "condition 后缀同上\n",
    "\n",
    "side 后缀：\n",
    "- SR 代表 side=right\n",
    "- SL 代表 side=left\n",
    "\n",
    "frequency 后缀：\n",
    "- FT 代表 frequency=theta\n",
    "- FA 代表 frequency=alpha\n",
    "- FG 代表 frequency=gamma\n",
    "- FLB 代表 frequency=low_beta\n",
    "- FHB 代表 frequency=high_beta\n",
    "\n",
    "新特征名：\n",
    "原特征名_condition后缀_side后缀_frequency后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c8874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义后缀映射\n",
    "condition_suffix = {\"Overground\": \"ConOg\", \"Treadmill\": \"ConTm\"}\n",
    "\n",
    "side_suffix = {\"right\": \"SR\", \"left\": \"SL\"}\n",
    "\n",
    "frequency_suffix = {\n",
    "    \"theta\": \"FT\",\n",
    "    \"alpha\": \"FA\",\n",
    "    \"gamma\": \"FG\",\n",
    "    \"low_beta\": \"FLB\",\n",
    "    \"high_beta\": \"FHB\",\n",
    "}\n",
    "\n",
    "# 获取需要重命名的列（除了pid、Group、condition、side、frequency）\n",
    "columns_to_rename = [\n",
    "    col\n",
    "    for col in spec_stats.columns\n",
    "    if col not in [\"pid\", \"group\", \"condition\", \"side\", \"frequency\"]\n",
    "]\n",
    "\n",
    "# 创建新的DataFrame来存储重构后的数据\n",
    "spec_stats_list = []\n",
    "\n",
    "# 遍历所有condition、side、frequency的组合\n",
    "for condition in spec_stats[\"condition\"].unique():\n",
    "    for side in spec_stats[\"side\"].unique():\n",
    "        for frequency in spec_stats[\"frequency\"].unique():\n",
    "            # 筛选特定组合的数据\n",
    "            subset = spec_stats[\n",
    "                (spec_stats[\"condition\"] == condition)\n",
    "                & (spec_stats[\"side\"] == side)\n",
    "                & (spec_stats[\"frequency\"] == frequency)\n",
    "            ].copy()\n",
    "\n",
    "            if not subset.empty:\n",
    "                # 删除condition、side、frequency列\n",
    "                subset = subset.drop([\"condition\", \"side\", \"frequency\"], axis=1)\n",
    "\n",
    "                # 构建后缀\n",
    "                suffix = f\"_{condition_suffix[condition]}_{side_suffix[side]}_{frequency_suffix[frequency]}\"\n",
    "\n",
    "                # 为需要重命名的列添加后缀\n",
    "                rename_dict = {col: f\"{col}{suffix}\" for col in columns_to_rename}\n",
    "                subset = subset.rename(columns=rename_dict)\n",
    "\n",
    "                spec_stats_list.append(subset)\n",
    "\n",
    "# 基于pid合并所有数据\n",
    "spec_stats = spec_stats_list[0]\n",
    "for df in spec_stats_list[1:]:\n",
    "    spec_stats = pd.merge(\n",
    "        spec_stats,\n",
    "        df[[\"pid\"] + [col for col in df.columns if col not in [\"pid\", \"group\"]]],\n",
    "        on=\"pid\",\n",
    "        how=\"outer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81ac2b",
   "metadata": {},
   "source": [
    "## tempo_stats 时间维度数据\n",
    "\n",
    "condition 和 side 后缀同上\n",
    "\n",
    "time 后缀：\n",
    "- TL 代表 time=lift\n",
    "- TD 代表 time=drop\n",
    "\n",
    "新特征名：\n",
    "原特征名_condition后缀_side后缀_time后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1479090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义后缀映射\n",
    "condition_suffix = {\"Overground\": \"ConOg\", \"Treadmill\": \"ConTm\"}\n",
    "\n",
    "side_suffix = {\"right\": \"SR\", \"left\": \"SL\"}\n",
    "\n",
    "time_suffix = {\n",
    "    \"lift\": \"TL\",\n",
    "    \"drop\": \"TD\",\n",
    "}\n",
    "\n",
    "# 获取需要重命名的列（除了pid、Group、condition、side、time）\n",
    "columns_to_rename = [\n",
    "    col\n",
    "    for col in tempo_stats.columns\n",
    "    if col not in [\"pid\", \"group\", \"condition\", \"side\", \"time\"]\n",
    "]\n",
    "\n",
    "# 创建新的DataFrame来存储重构后的数据\n",
    "tempo_stats_list = []\n",
    "\n",
    "# 遍历所有condition、side、time的组合\n",
    "for condition in tempo_stats[\"condition\"].unique():\n",
    "    for side in tempo_stats[\"side\"].unique():\n",
    "        for time in tempo_stats[\"time\"].unique():\n",
    "            # 筛选特定组合的数据\n",
    "            subset = tempo_stats[\n",
    "                (tempo_stats[\"condition\"] == condition)\n",
    "                & (tempo_stats[\"side\"] == side)\n",
    "                & (tempo_stats[\"time\"] == time)\n",
    "            ].copy()\n",
    "\n",
    "            if not subset.empty:\n",
    "                # 删除condition、side、time列\n",
    "                subset = subset.drop([\"condition\", \"side\", \"time\"], axis=1)\n",
    "\n",
    "                # 构建后缀\n",
    "                suffix = f\"_{condition_suffix[condition]}_{side_suffix[side]}_{time_suffix[time]}\"\n",
    "\n",
    "                # 为需要重命名的列添加后缀\n",
    "                rename_dict = {col: f\"{col}{suffix}\" for col in columns_to_rename}\n",
    "                subset = subset.rename(columns=rename_dict)\n",
    "\n",
    "                tempo_stats_list.append(subset)\n",
    "\n",
    "# 基于pid合并所有数据\n",
    "tempo_stats = tempo_stats_list[0]\n",
    "for df in tempo_stats_list[1:]:\n",
    "    tempo_stats = pd.merge(\n",
    "        tempo_stats,\n",
    "        df[[\"pid\"] + [col for col in df.columns if col not in [\"pid\", \"group\"]]],\n",
    "        on=\"pid\",\n",
    "        how=\"outer\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219d100",
   "metadata": {},
   "source": [
    "## 合并所有数据（除帕金森特征外）为 data 表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b266805",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_stats = tempo_stats.rename(columns={\"pid\": \"ID\", \"group\": \"Group\"})\n",
    "spec_stats = spec_stats.rename(columns={\"pid\": \"ID\", \"group\": \"Group\"})\n",
    "\n",
    "# 步骤1: 先合并gait_para和spec_stats\n",
    "data = pd.merge(\n",
    "    gait_para,\n",
    "    spec_stats[\n",
    "        [\"ID\"] + [col for col in spec_stats.columns if col not in [\"ID\", \"Group\"]]\n",
    "    ],\n",
    "    on=\"ID\",\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "# 步骤2: 合并tempo_stats\n",
    "data = pd.merge(\n",
    "    data,\n",
    "    tempo_stats[\n",
    "        [\"ID\"] + [col for col in tempo_stats.columns if col not in [\"ID\", \"Group\"]]\n",
    "    ],\n",
    "    on=\"ID\",\n",
    "    how=\"outer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4176cb2d",
   "metadata": {},
   "source": [
    "# 数据类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f706f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== data 表格列类型 ===\n",
      "ID                        int64\n",
      "Group                    object\n",
      "stridetime_ConOg        float64\n",
      "steptime_ConOg          float64\n",
      "stridetimevari_ConOg    float64\n",
      "                         ...   \n",
      "emg_env_ConOg_SR_TD     float64\n",
      "emg_env_ConTm_SL_TL     float64\n",
      "emg_env_ConTm_SL_TD     float64\n",
      "emg_env_ConTm_SR_TL     float64\n",
      "emg_env_ConTm_SR_TD     float64\n",
      "Length: 188, dtype: object\n",
      "\n",
      "数据形状: (66, 188)\n",
      "\n",
      "=== 按数据类型分组的列名 ===\n",
      "int64: 1 列\n",
      "   ['ID']\n",
      "object: 3 列\n",
      "   ['Group', 'walkingspeed_ConOg', 'walkingspeed_ConTm']\n",
      "float64: 184 列\n"
     ]
    }
   ],
   "source": [
    "# 检查合并后数据的列类型\n",
    "print(\"=== data 表格列类型 ===\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\n数据形状: {data.shape}\")\n",
    "\n",
    "# 按数据类型分组查看列名\n",
    "print(\"\\n=== 按数据类型分组的列名 ===\")\n",
    "for dtype in data.dtypes.unique():\n",
    "    cols = data.select_dtypes(include=[dtype]).columns.tolist()\n",
    "    print(f\"{dtype}: {len(cols)} 列\")\n",
    "    if len(cols) < 10:  # 如果列数不多，显示列名\n",
    "        print(f\"   {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ccb23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要转换的列: ['walkingspeed_ConOg', 'walkingspeed_ConTm']\n",
      "\n",
      "=== 转换后的数据类型 ===\n",
      "float64    186\n",
      "int64        1\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "剩余的object类型列: ['Group']\n"
     ]
    }
   ],
   "source": [
    "# 将除Group外的object列转换为float64\n",
    "object_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "cols_to_convert = [col for col in object_cols if col != \"Group\"]\n",
    "\n",
    "print(f\"需要转换的列: {cols_to_convert}\")\n",
    "\n",
    "# 转换数据类型，使用errors='coerce'将无法转换的值设为NaN\n",
    "for col in cols_to_convert:\n",
    "    data[col] = pd.to_numeric(data[col], errors=\"coerce\")\n",
    "\n",
    "# 验证转换结果\n",
    "print(\"\\n=== 转换后的数据类型 ===\")\n",
    "print(data.dtypes.value_counts())\n",
    "\n",
    "# 检查是否还有object类型的列（除了Group）\n",
    "print(\n",
    "    f\"\\n剩余的object类型列: {data.select_dtypes(include=['object']).columns.tolist()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257aad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分出独立的帕金森病人表格\n",
    "data_PD = data[(data[\"Group\"] == \"Parkinson\")].copy()\n",
    "\n",
    "# 将帕金森特征并入帕金森病人表格\n",
    "data_PD = pd.merge(\n",
    "    data_PD,\n",
    "    PD_characteristics[\n",
    "        [\"ID\"]\n",
    "        + [col for col in PD_characteristics.columns if col not in [\"ID\", \"Group\"]]\n",
    "    ],\n",
    "    on=\"ID\",\n",
    "    how=\"outer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ebbe6",
   "metadata": {},
   "source": [
    "# 帕金森评级"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747629e5",
   "metadata": {},
   "source": [
    "根据 H&Y 分级对 PD 病人评级：\n",
    "- H&Y 为 1：Group 设置为 PD_H&Y1\n",
    "- H&Y 大于 1 小于 3：Group 设置为 PD_H&Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5f42ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoehnYahrON 值分布:\n",
      "HoehnYahrON\n",
      "1.0    15\n",
      "2.0     4\n",
      "2.5     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HoehnYahrON 列的数据类型: float64\n",
      "缺失值数量: 0\n",
      "\n",
      "=== H&Y 分级结果 ===\n",
      "Group\n",
      "PD_H&Y1    15\n",
      "PD_H&Y2     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "各组的 HoehnYahrON 值范围:\n",
      "PD_H&Y1: 1.0 - 1.0\n",
      "PD_H&Y2: 2.0 - 2.5\n"
     ]
    }
   ],
   "source": [
    "# 检查 HoehnYahrON 列的值分布\n",
    "print(\"HoehnYahrON 值分布:\")\n",
    "print(data_PD[\"HoehnYahrON\"].value_counts().sort_index())\n",
    "print(f\"\\nHoehnYahrON 列的数据类型: {data_PD['HoehnYahrON'].dtype}\")\n",
    "print(f\"缺失值数量: {data_PD['HoehnYahrON'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "# 根据 H&Y 分级对 PD 病人评级\n",
    "def classify_by_HY(hy_score):\n",
    "    if pd.isna(hy_score):\n",
    "        return \"PD_Unknown\"  # 处理缺失值\n",
    "    elif hy_score == 1:\n",
    "        return \"PD_H&Y1\"\n",
    "    elif 1 < hy_score < 3:\n",
    "        return \"PD_H&Y2\"\n",
    "    else:\n",
    "        return \"PD_Other\"  # 处理其他值（如 ≥3 的情况）\n",
    "\n",
    "\n",
    "data_PD[\"Group\"] = data_PD[\"HoehnYahrON\"].apply(classify_by_HY)\n",
    "\n",
    "# 检查分组结果\n",
    "print(\"\\n=== H&Y 分级结果 ===\")\n",
    "print(data_PD[\"Group\"].value_counts())\n",
    "print(\"\\n各组的 HoehnYahrON 值范围:\")\n",
    "for group in data_PD[\"Group\"].unique():\n",
    "    subset = data_PD[data_PD[\"Group\"] == group][\"HoehnYahrON\"]\n",
    "    print(f\"{group}: {subset.min()} - {subset.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25849789",
   "metadata": {},
   "source": [
    "对 data 中病人进行同样的分级"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7413f2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 更新后的 Group 分布 ===\n",
      "Group\n",
      "Old        24\n",
      "Young      22\n",
      "PD_H&Y1    15\n",
      "PD_H&Y2     5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 根据 ID 值，用 data_PD 的 Group 列覆盖 data 的 Group 列\n",
    "# 创建一个映射字典，将 data_PD 中的 ID 和新的 Group 对应起来\n",
    "pd_group_mapping = dict(zip(data_PD[\"ID\"], data_PD[\"Group\"]))\n",
    "\n",
    "# 更新 data 中对应 ID 的 Group 值\n",
    "data[\"Group\"] = data.apply(\n",
    "    lambda row: pd_group_mapping.get(row[\"ID\"], row[\"Group\"]), axis=1\n",
    ")\n",
    "\n",
    "# 验证更新结果\n",
    "print(\"=== 更新后的 Group 分布 ===\")\n",
    "print(data[\"Group\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea264db",
   "metadata": {},
   "source": [
    "# 训练-测试集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63de501a",
   "metadata": {},
   "source": [
    "后续处理都将只处理 data 数据集，不管 data_PD 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03fc183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据标签分布:\n",
      "Group\n",
      "Old        24\n",
      "Young      22\n",
      "PD_H&Y1    15\n",
      "PD_H&Y2     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "训练集标签分布:\n",
      "Group\n",
      "Old        19\n",
      "Young      17\n",
      "PD_H&Y1    12\n",
      "PD_H&Y2     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "测试集标签分布:\n",
      "Group\n",
      "Young      5\n",
      "Old        5\n",
      "PD_H&Y1    3\n",
      "PD_H&Y2    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 设置分割器参数\n",
    "splitter = StratifiedShuffleSplit(\n",
    "    n_splits=1,  # 只需要一次划分\n",
    "    test_size=0.2,  # 20%作为测试集\n",
    "    random_state=15,  # 保证结果可重现\n",
    ")\n",
    "\n",
    "# 准备特征和标签\n",
    "X = data.drop([\"ID\", \"Group\"], axis=1)  # 所有特征\n",
    "y = data[\"Group\"]  # 标签（组别）\n",
    "\n",
    "# 执行分层划分\n",
    "train_index, test_index = next(splitter.split(X, y))\n",
    "\n",
    "# 创建训练集和测试集\n",
    "X_train = X.iloc[train_index]\n",
    "X_test = X.iloc[test_index]\n",
    "y_train = y.iloc[train_index]\n",
    "y_test = y.iloc[test_index]\n",
    "\n",
    "# 获取对应的ID\n",
    "train_ids = data.loc[train_index, \"ID\"]\n",
    "test_ids = data.loc[test_index, \"ID\"]\n",
    "\n",
    "# 验证分层效果\n",
    "print(\"原始数据标签分布:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\n训练集标签分布:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\n测试集标签分布:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56f516",
   "metadata": {},
   "source": [
    "# 数据清洗和缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2de31ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 训练集缺失值情况 ===\n",
      "训练集形状: (52, 186)\n",
      "总缺失值数量: 91\n",
      "缺失值比例: 0.0094\n",
      "\n",
      "=== 测试集缺失值情况 ===\n",
      "测试集形状: (14, 186)\n",
      "总缺失值数量: 85\n",
      "缺失值比例: 0.0326\n",
      "\n",
      "=== 有缺失值的列统计 ===\n",
      "训练集有缺失值的列数: 86\n",
      "测试集有缺失值的列数: 85\n",
      "\n",
      "训练集缺失值最多的前5列:\n",
      "  walkingspeed_ConOg: 5 (0.096)\n",
      "  walkingspeed_ConTm: 2 (0.038)\n",
      "  itc_emg_ConTm_SL_FT: 1 (0.019)\n",
      "  cmc_ConTm_SL_FT: 1 (0.019)\n",
      "  z_itc_eeg_ConTm_SL_FLB: 1 (0.019)\n",
      "\n",
      "测试集缺失值最多的前5列:\n",
      "  walkingspeed_ConOg: 1 (0.071)\n",
      "  z_itc_eeg_ConTm_SL_FA: 1 (0.071)\n",
      "  z_itc_eeg_ConTm_SL_FLB: 1 (0.071)\n",
      "  cmc_ConTm_SL_FLB: 1 (0.071)\n",
      "  itc_emg_ConTm_SL_FLB: 1 (0.071)\n"
     ]
    }
   ],
   "source": [
    "# 查看训练集和测试集的缺失值情况\n",
    "print(\"=== 训练集缺失值情况 ===\")\n",
    "print(f\"训练集形状: {X_train.shape}\")\n",
    "print(f\"总缺失值数量: {X_train.isnull().sum().sum()}\")\n",
    "print(\n",
    "    f\"缺失值比例: {X_train.isnull().sum().sum() / (X_train.shape[0] * X_train.shape[1]):.4f}\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== 测试集缺失值情况 ===\")\n",
    "print(f\"测试集形状: {X_test.shape}\")\n",
    "print(f\"总缺失值数量: {X_test.isnull().sum().sum()}\")\n",
    "print(\n",
    "    f\"缺失值比例: {X_test.isnull().sum().sum() / (X_test.shape[0] * X_test.shape[1]):.4f}\"\n",
    ")\n",
    "\n",
    "# 查看有缺失值的列\n",
    "train_missing = X_train.isnull().sum()\n",
    "test_missing = X_test.isnull().sum()\n",
    "\n",
    "print(\"\\n=== 有缺失值的列统计 ===\")\n",
    "print(f\"训练集有缺失值的列数: {sum(train_missing > 0)}\")\n",
    "print(f\"测试集有缺失值的列数: {sum(test_missing > 0)}\")\n",
    "\n",
    "# 显示缺失值最多的前5列\n",
    "if sum(train_missing > 0) > 0:\n",
    "    print(\"\\n训练集缺失值最多的前5列:\")\n",
    "    top_missing_train = (\n",
    "        train_missing[train_missing > 0].sort_values(ascending=False).head(5)\n",
    "    )\n",
    "    for col, count in top_missing_train.items():\n",
    "        print(f\"  {col}: {count} ({count / len(X_train):.3f})\")\n",
    "\n",
    "if sum(test_missing > 0) > 0:\n",
    "    print(\"\\n测试集缺失值最多的前5列:\")\n",
    "    top_missing_test = (\n",
    "        test_missing[test_missing > 0].sort_values(ascending=False).head(5)\n",
    "    )\n",
    "    for col, count in top_missing_test.items():\n",
    "        print(f\"  {col}: {count} ({count / len(X_test):.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7270b4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 训练集离群值分析 ===\n",
      "训练集形状: (52, 186)\n",
      "有离群值的特征数: 123\n",
      "总离群值数量: 315\n",
      "离群值比例: 0.0326\n",
      "\n",
      "离群值最多的前5个特征:\n",
      "  cmc_ConTm_SR_FA: 9 (0.173)\n",
      "  power_eeg_ConTm_SR_FA: 7 (0.135)\n",
      "  itc_eeg_ConTm_SL_FG: 6 (0.115)\n",
      "  itc_eeg_ConTm_SR_FG: 6 (0.115)\n",
      "  cmc_ConOg_SL_FG: 6 (0.115)\n",
      "\n",
      "=== 测试集离群值分析 ===\n",
      "测试集形状: (14, 186)\n",
      "有离群值的特征数: 75\n",
      "总离群值数量: 105\n",
      "离群值比例: 0.0403\n",
      "\n",
      "离群值最多的前5个特征:\n",
      "  z_itc_emg_ConOg_SR_FT: 5 (0.357)\n",
      "  itc_emg_ConOg_SR_FT: 5 (0.357)\n",
      "  power_eeg_ConTm_SR_FT: 3 (0.214)\n",
      "  power_eeg_ConOg_SL_FA: 3 (0.214)\n",
      "  emg_env_ConTm_SR_TD: 2 (0.143)\n",
      "\n",
      "=== 样本层面离群值统计 ===\n",
      "训练集样本离群值统计:\n",
      "  平均每个样本有 6.06 个离群特征\n",
      "  最多离群特征数: 39\n",
      "测试集样本离群值统计:\n",
      "  平均每个样本有 7.50 个离群特征\n",
      "  最多离群特征数: 21\n"
     ]
    }
   ],
   "source": [
    "# 简单分析训练集和测试集的离群值情况\n",
    "def detect_outliers_iqr(data):\n",
    "    \"\"\"使用IQR方法检测离群值\"\"\"\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # 统计每列的离群值数量\n",
    "    outliers_count = ((data < lower_bound) | (data > upper_bound)).sum()\n",
    "    return outliers_count\n",
    "\n",
    "\n",
    "print(\"=== 训练集离群值分析 ===\")\n",
    "train_outliers = detect_outliers_iqr(X_train)\n",
    "train_outliers_nonzero = train_outliers[train_outliers > 0]\n",
    "\n",
    "print(f\"训练集形状: {X_train.shape}\")\n",
    "print(f\"有离群值的特征数: {len(train_outliers_nonzero)}\")\n",
    "print(f\"总离群值数量: {train_outliers.sum()}\")\n",
    "print(f\"离群值比例: {train_outliers.sum() / (X_train.shape[0] * X_train.shape[1]):.4f}\")\n",
    "\n",
    "if len(train_outliers_nonzero) > 0:\n",
    "    print(\"\\n离群值最多的前5个特征:\")\n",
    "    top_outliers_train = train_outliers_nonzero.sort_values(ascending=False).head(5)\n",
    "    for col, count in top_outliers_train.items():\n",
    "        print(f\"  {col}: {count} ({count / len(X_train):.3f})\")\n",
    "\n",
    "print(\"\\n=== 测试集离群值分析 ===\")\n",
    "test_outliers = detect_outliers_iqr(X_test)\n",
    "test_outliers_nonzero = test_outliers[test_outliers > 0]\n",
    "\n",
    "print(f\"测试集形状: {X_test.shape}\")\n",
    "print(f\"有离群值的特征数: {len(test_outliers_nonzero)}\")\n",
    "print(f\"总离群值数量: {test_outliers.sum()}\")\n",
    "print(f\"离群值比例: {test_outliers.sum() / (X_test.shape[0] * X_test.shape[1]):.4f}\")\n",
    "\n",
    "if len(test_outliers_nonzero) > 0:\n",
    "    print(\"\\n离群值最多的前5个特征:\")\n",
    "    top_outliers_test = test_outliers_nonzero.sort_values(ascending=False).head(5)\n",
    "    for col, count in top_outliers_test.items():\n",
    "        print(f\"  {col}: {count} ({count / len(X_test):.3f})\")\n",
    "\n",
    "# 按样本统计离群值\n",
    "print(\"\\n=== 样本层面离群值统计 ===\")\n",
    "train_sample_outliers = (\n",
    "    (\n",
    "        X_train\n",
    "        < X_train.quantile(0.25)\n",
    "        - 1.5 * (X_train.quantile(0.75) - X_train.quantile(0.25))\n",
    "    )\n",
    "    | (\n",
    "        X_train\n",
    "        > X_train.quantile(0.75)\n",
    "        + 1.5 * (X_train.quantile(0.75) - X_train.quantile(0.25))\n",
    "    )\n",
    ").sum(axis=1)\n",
    "\n",
    "test_sample_outliers = (\n",
    "    (\n",
    "        X_test\n",
    "        < X_test.quantile(0.25) - 1.5 * (X_test.quantile(0.75) - X_test.quantile(0.25))\n",
    "    )\n",
    "    | (\n",
    "        X_test\n",
    "        > X_test.quantile(0.75) + 1.5 * (X_test.quantile(0.75) - X_test.quantile(0.25))\n",
    "    )\n",
    ").sum(axis=1)\n",
    "\n",
    "print(\"训练集样本离群值统计:\")\n",
    "print(f\"  平均每个样本有 {train_sample_outliers.mean():.2f} 个离群特征\")\n",
    "print(f\"  最多离群特征数: {train_sample_outliers.max()}\")\n",
    "\n",
    "print(\"测试集样本离群值统计:\")\n",
    "print(f\"  平均每个样本有 {test_sample_outliers.mean():.2f} 个离群特征\")\n",
    "print(f\"  最多离群特征数: {test_sample_outliers.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8855b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 预处理结果验证 ===\n",
      "训练集形状: (52, 186)\n",
      "测试集形状: (14, 186)\n",
      "训练集缺失值: 0\n",
      "测试集缺失值: 0\n",
      "\n",
      "=== 标准化效果验证 ===\n",
      "训练集统计:\n",
      "  均值范围: [-0.000000, 0.000000]\n",
      "  标准差范围: [1.009756, 1.009756]\n",
      "测试集统计:\n",
      "  均值范围: [-0.718106, 1.916185]\n",
      "  标准差范围: [0.142693, 5.663188]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 创建预处理管道\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    创建包含KNN缺失值填补和标准化的预处理管道\n",
    "    \"\"\"\n",
    "    # 数值特征预处理管道：KNN填补 + 标准化\n",
    "    numeric_pipeline = Pipeline(\n",
    "        [(\"imputer\", KNNImputer(n_neighbors=5)), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    # 获取所有数值特征列名\n",
    "    numeric_features = X_train.columns.tolist()\n",
    "\n",
    "    # 创建ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"numeric\", numeric_pipeline, numeric_features)],\n",
    "        remainder=\"passthrough\",  # 保留其他列（如果有的话）\n",
    "    )\n",
    "\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "# 创建预处理器\n",
    "preprocessor = create_preprocessing_pipeline()\n",
    "\n",
    "# 训练预处理器并转换训练集\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# 转换测试集（使用训练集学到的参数）\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# 将结果转换回DataFrame格式，保持列名\n",
    "X_train_clean = pd.DataFrame(\n",
    "    X_train_processed, columns=X_train.columns, index=X_train.index\n",
    ")\n",
    "\n",
    "X_test_clean = pd.DataFrame(\n",
    "    X_test_processed, columns=X_test.columns, index=X_test.index\n",
    ")\n",
    "\n",
    "# 验证预处理结果\n",
    "print(\"=== 预处理结果验证 ===\")\n",
    "print(f\"训练集形状: {X_train_clean.shape}\")\n",
    "print(f\"测试集形状: {X_test_clean.shape}\")\n",
    "print(f\"训练集缺失值: {X_train_clean.isnull().sum().sum()}\")\n",
    "print(f\"测试集缺失值: {X_test_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# 验证标准化效果\n",
    "print(\"\\n=== 标准化效果验证 ===\")\n",
    "print(\"训练集统计:\")\n",
    "print(\n",
    "    f\"  均值范围: [{X_train_clean.mean().min():.6f}, {X_train_clean.mean().max():.6f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"  标准差范围: [{X_train_clean.std().min():.6f}, {X_train_clean.std().max():.6f}]\"\n",
    ")\n",
    "print(\"测试集统计:\")\n",
    "print(f\"  均值范围: [{X_test_clean.mean().min():.6f}, {X_test_clean.mean().max():.6f}]\")\n",
    "print(f\"  标准差范围: [{X_test_clean.std().min():.6f}, {X_test_clean.std().max():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126594f",
   "metadata": {},
   "source": [
    "# 数据降维"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc94d2",
   "metadata": {},
   "source": [
    "对特征进行分组后，分别进行降维，这样的主成分生物学意义更清晰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106a371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 特征分组分析 ===\n",
      "特征分组统计:\n",
      "  gait_features: 18 个特征\n",
      "  spectral_features: 160 个特征\n",
      "  temporal_features: 8 个特征\n",
      "  未分组特征: 0 个\n",
      "  总特征数: 186\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "# 分析特征分组情况\n",
    "def analyze_feature_groups(X_train_clean):\n",
    "    \"\"\"分析数据中的特征分组\"\"\"\n",
    "\n",
    "    print(\"=== 特征分组分析 ===\")\n",
    "\n",
    "    all_features = X_train_clean.columns.tolist()\n",
    "\n",
    "    # 分组策略\n",
    "    feature_groups = {\n",
    "        \"gait_features\": [],  # 所有步态特征\n",
    "        \"spectral_features\": [],  # 所有频谱特征\n",
    "        \"temporal_features\": [],  # 所有时域特征\n",
    "    }\n",
    "\n",
    "    for feature in all_features:\n",
    "        if any(freq in feature for freq in [\"FT\", \"FA\", \"FG\", \"FLB\", \"FHB\"]):\n",
    "            feature_groups[\"spectral_features\"].append(feature)\n",
    "        elif any(time in feature for time in [\"TL\", \"TD\"]):\n",
    "            feature_groups[\"temporal_features\"].append(feature)\n",
    "        elif any(condition in feature for condition in [\"ConOg\", \"ConTm\"]):\n",
    "            feature_groups[\"gait_features\"].append(feature)\n",
    "\n",
    "    # 显示分组结果\n",
    "    print(\"特征分组统计:\")\n",
    "    total_grouped = 0\n",
    "    for group_name, features in feature_groups.items():\n",
    "        if features:\n",
    "            print(f\"  {group_name}: {len(features)} 个特征\")\n",
    "            total_grouped += len(features)\n",
    "\n",
    "    ungrouped = len(all_features) - total_grouped\n",
    "    print(f\"  未分组特征: {ungrouped} 个\")\n",
    "    print(f\"  总特征数: {len(all_features)}\")\n",
    "\n",
    "    return feature_groups\n",
    "\n",
    "\n",
    "# 执行特征分组分析\n",
    "feature_groups = analyze_feature_groups(X_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 基于解释方差率的分组降维 ===\n",
      "目标解释方差率: 90.0%\n",
      "\n",
      "--- 处理 gait_features ---\n",
      "原始特征数: 18\n",
      "特征选择后: 14 个特征\n",
      "目标方差解释率: 90.0%\n",
      "实际方差解释率: 92.2%\n",
      "选择的主成分数: 5\n",
      "特征压缩比: 0.278\n",
      "\n",
      "--- 处理 spectral_features ---\n",
      "原始特征数: 160\n",
      "特征选择后: 128 个特征\n",
      "目标方差解释率: 90.0%\n",
      "实际方差解释率: 90.6%\n",
      "选择的主成分数: 20\n",
      "特征压缩比: 0.125\n",
      "\n",
      "--- 处理 temporal_features ---\n",
      "原始特征数: 8\n",
      "特征选择后: 8 个特征\n",
      "目标方差解释率: 90.0%\n",
      "实际方差解释率: 91.4%\n",
      "选择的主成分数: 4\n",
      "特征压缩比: 0.500\n",
      "\n",
      "=== 分组降维总结 ===\n",
      "原始总特征数: 186\n",
      "降维后总特征数: 29\n",
      "总体压缩比: 0.156\n",
      "总体压缩率: 84.4%\n"
     ]
    }
   ],
   "source": [
    "# 实现基于解释方差率的分组降维\n",
    "def grouped_dimensionality_reduction_by_variance(\n",
    "    X_train,\n",
    "    X_test,\n",
    "    y_train,\n",
    "    feature_groups,\n",
    "    target_variance_ratio=0.90,\n",
    "    min_components_per_group=3,\n",
    "    max_components_per_group=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    按特征组别分别进行降维，基于解释方差率确定降维数量\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : DataFrame\n",
    "        训练集和测试集特征\n",
    "    y_train : Series\n",
    "        训练集标签\n",
    "    feature_groups : dict\n",
    "        特征分组字典\n",
    "    target_variance_ratio : float\n",
    "        目标解释方差比例 (默认0.90 = 90%)\n",
    "    min_components_per_group : int\n",
    "        每组最少保留的主成分数\n",
    "    max_components_per_group : int or None\n",
    "        每组最多保留的主成分数 (None表示不限制)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== 基于解释方差率的分组降维 ===\")\n",
    "    print(f\"目标解释方差率: {target_variance_ratio:.1%}\")\n",
    "\n",
    "    reduced_train_parts = []\n",
    "    reduced_test_parts = []\n",
    "    reduction_info = {}\n",
    "\n",
    "    for group_name, features in feature_groups.items():\n",
    "        if not features:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- 处理 {group_name} ---\")\n",
    "        print(f\"原始特征数: {len(features)}\")\n",
    "\n",
    "        # 提取该组特征\n",
    "        X_train_group = X_train[features]\n",
    "        X_test_group = X_test[features]\n",
    "\n",
    "        # 步骤1: 特征选择 (选择与目标变量最相关的特征)\n",
    "        # 选择前80%的特征，但至少保留10个，最多不超过原特征数\n",
    "        n_select = min(max(int(len(features) * 0.8), 10), len(features))\n",
    "\n",
    "        selector = SelectKBest(score_func=f_classif, k=n_select)\n",
    "        X_train_selected = selector.fit_transform(X_train_group, y_train)\n",
    "        X_test_selected = selector.transform(X_test_group)\n",
    "\n",
    "        selected_features = np.array(features)[selector.get_support()]\n",
    "        print(f\"特征选择后: {len(selected_features)} 个特征\")\n",
    "\n",
    "        # 步骤2: 确定PCA组件数量\n",
    "        # 首先用所有可能的组件训练PCA来分析方差解释情况\n",
    "        max_possible_components = min(len(selected_features), len(X_train) - 1)\n",
    "\n",
    "        pca_full = PCA(n_components=max_possible_components)\n",
    "        pca_full.fit(X_train_selected)\n",
    "\n",
    "        # 计算累积解释方差比例\n",
    "        cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "        # 找到达到目标解释方差率的组件数\n",
    "        n_components = np.argmax(cumulative_variance >= target_variance_ratio) + 1\n",
    "\n",
    "        # 应用约束条件\n",
    "        if max_components_per_group is not None:\n",
    "            n_components = min(n_components, max_components_per_group)\n",
    "        n_components = max(n_components, min_components_per_group)\n",
    "        n_components = min(n_components, max_possible_components)\n",
    "\n",
    "        # 实际方差解释率\n",
    "        actual_variance_ratio = cumulative_variance[n_components - 1]\n",
    "\n",
    "        print(f\"目标方差解释率: {target_variance_ratio:.1%}\")\n",
    "        print(f\"实际方差解释率: {actual_variance_ratio:.1%}\")\n",
    "        print(f\"选择的主成分数: {n_components}\")\n",
    "\n",
    "        # 步骤3: 使用确定的组件数重新训练PCA\n",
    "        pca_final = PCA(n_components=n_components)\n",
    "        X_train_pca = pca_final.fit_transform(X_train_selected)\n",
    "        X_test_pca = pca_final.transform(X_test_selected)\n",
    "\n",
    "        # 创建新的列名\n",
    "        new_columns = [f\"{group_name}_PC{i + 1}\" for i in range(n_components)]\n",
    "\n",
    "        # 转换为DataFrame\n",
    "        train_df = pd.DataFrame(X_train_pca, columns=new_columns, index=X_train.index)\n",
    "        test_df = pd.DataFrame(X_test_pca, columns=new_columns, index=X_test.index)\n",
    "\n",
    "        reduced_train_parts.append(train_df)\n",
    "        reduced_test_parts.append(test_df)\n",
    "\n",
    "        # 保存降维信息\n",
    "        reduction_info[group_name] = {\n",
    "            \"original_features\": len(features),\n",
    "            \"selected_features\": len(selected_features),\n",
    "            \"final_components\": n_components,\n",
    "            \"target_variance_ratio\": target_variance_ratio,\n",
    "            \"actual_variance_ratio\": actual_variance_ratio,\n",
    "            \"individual_variance_ratios\": pca_final.explained_variance_ratio_,\n",
    "            \"cumulative_variance_ratios\": np.cumsum(\n",
    "                pca_final.explained_variance_ratio_\n",
    "            ),\n",
    "            \"selected_feature_names\": selected_features.tolist(),\n",
    "            \"feature_scores\": selector.scores_[selector.get_support()],\n",
    "            \"compression_ratio\": n_components / len(features),\n",
    "        }\n",
    "\n",
    "        print(f\"特征压缩比: {n_components / len(features):.3f}\")\n",
    "\n",
    "    # 合并所有组的降维结果\n",
    "    X_train_reduced = pd.concat(reduced_train_parts, axis=1)\n",
    "    X_test_reduced = pd.concat(reduced_test_parts, axis=1)\n",
    "\n",
    "    # 计算总体统计信息\n",
    "    total_original_features = sum(\n",
    "        [info[\"original_features\"] for info in reduction_info.values()]\n",
    "    )\n",
    "    total_final_features = X_train_reduced.shape[1]\n",
    "    overall_compression = total_final_features / total_original_features\n",
    "\n",
    "    print(\"\\n=== 分组降维总结 ===\")\n",
    "    print(f\"原始总特征数: {total_original_features}\")\n",
    "    print(f\"降维后总特征数: {total_final_features}\")\n",
    "    print(f\"总体压缩比: {overall_compression:.3f}\")\n",
    "    print(f\"总体压缩率: {(1 - overall_compression):.1%}\")\n",
    "\n",
    "    return X_train_reduced, X_test_reduced, reduction_info\n",
    "\n",
    "\n",
    "# 执行基于解释方差率的分组降维\n",
    "X_train_reduced, X_test_reduced, group_reduction_info = (\n",
    "    grouped_dimensionality_reduction_by_variance(\n",
    "        X_train_clean,\n",
    "        X_test_clean,\n",
    "        y_train,\n",
    "        feature_groups,\n",
    "        target_variance_ratio=0.90,  # 90%解释方差率\n",
    "        min_components_per_group=3,  # 每组最少3个主成分\n",
    "        max_components_per_group=20,  # 每组最多20个主成分\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb868c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 最终降维结果总结 ===\n",
      "采用解释方差率: 90%\n",
      "原始总特征数: 186\n",
      "降维后总特征数: 29\n",
      "特征减少数量: 157\n",
      "总体压缩率: 84.4%\n",
      "\n",
      "各组降维结果:\n",
      "  gait_features:\n",
      "    18 → 5 维\n",
      "    解释方差率: 92.2%\n",
      "    压缩率: 72.2%\n",
      "  spectral_features:\n",
      "    160 → 20 维\n",
      "    解释方差率: 90.6%\n",
      "    压缩率: 87.5%\n",
      "  temporal_features:\n",
      "    8 → 4 维\n",
      "    解释方差率: 91.4%\n",
      "    压缩率: 50.0%\n",
      "\n",
      "降维后的数据已准备完成，可用于后续建模\n",
      "训练集形状: (52, 29)\n",
      "测试集形状: (14, 29)\n"
     ]
    }
   ],
   "source": [
    "# 最终降维结果总结\n",
    "print(\"=== 最终降维结果总结 ===\")\n",
    "print(\"采用解释方差率: 90%\")\n",
    "print(f\"原始总特征数: {X_train_clean.shape[1]}\")\n",
    "print(f\"降维后总特征数: {X_train_reduced.shape[1]}\")\n",
    "print(f\"特征减少数量: {X_train_clean.shape[1] - X_train_reduced.shape[1]}\")\n",
    "print(f\"总体压缩率: {(1 - X_train_reduced.shape[1] / X_train_clean.shape[1]):.1%}\")\n",
    "\n",
    "print(\"\\n各组降维结果:\")\n",
    "for group_name, info in group_reduction_info.items():\n",
    "    print(f\"  {group_name}:\")\n",
    "    print(f\"    {info['original_features']} → {info['final_components']} 维\")\n",
    "    print(f\"    解释方差率: {info['actual_variance_ratio']:.1%}\")\n",
    "    print(f\"    压缩率: {(1 - info['compression_ratio']):.1%}\")\n",
    "\n",
    "print(\"\\n降维后的数据已准备完成，可用于后续建模\")\n",
    "print(f\"训练集形状: {X_train_reduced.shape}\")\n",
    "print(f\"测试集形状: {X_test_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d5ad4f",
   "metadata": {},
   "source": [
    "# 导出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41663dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 导出数据到 PreprocessedData 目录 ===\n",
      "✓ X_train.csv 已保存 - 形状: (52, 186)\n",
      "✓ X_train_clean.csv 已保存 - 形状: (52, 186)\n",
      "✓ X_train_reduced.csv 已保存 - 形状: (52, 29)\n",
      "✓ y_train.csv 已保存 - 长度: 52\n",
      "✓ X_test.csv 已保存 - 形状: (14, 186)\n",
      "✓ X_test_clean.csv 已保存 - 形状: (14, 186)\n",
      "✓ X_test_reduced.csv 已保存 - 形状: (14, 29)\n",
      "✓ y_test.csv 已保存 - 长度: 14\n",
      "\n",
      "=== 导出ID信息 ===\n",
      "✓ train_ids.csv 已保存 - 长度: 52\n",
      "✓ test_ids.csv 已保存 - 长度: 14\n",
      "\n",
      "所有数据文件已成功导出到: e:\\文档\\重要文档\\2025.8\\剑桥PBL\\ML\\Diagnosis_PD_MultimodelGaitInfo\\PreprocessedData\n",
      "\n",
      "文件列表:\n",
      "  X_test.csv (38.3 KB)\n",
      "  X_test_clean.csv (53.9 KB)\n",
      "  X_test_reduced.csv (8.4 KB)\n",
      "  X_train.csv (135.0 KB)\n",
      "  X_train_clean.csv (189.8 KB)\n",
      "  X_train_reduced.csv (29.3 KB)\n",
      "  test_ids.csv (0.1 KB)\n",
      "  train_ids.csv (0.2 KB)\n",
      "  y_test.csv (0.1 KB)\n",
      "  y_train.csv (0.4 KB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 创建 PreprocessedData 目录（如果不存在）\n",
    "output_dir = \"./PreprocessedData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 导出训练集特征数据\n",
    "print(\"=== 导出数据到 PreprocessedData 目录 ===\")\n",
    "\n",
    "# 1. 原始训练集特征\n",
    "X_train.to_csv(os.path.join(output_dir, \"X_train.csv\"), index=False)\n",
    "print(f\"✓ X_train.csv 已保存 - 形状: {X_train.shape}\")\n",
    "\n",
    "# 2. 清洗后训练集特征\n",
    "X_train_clean.to_csv(os.path.join(output_dir, \"X_train_clean.csv\"), index=False)\n",
    "print(f\"✓ X_train_clean.csv 已保存 - 形状: {X_train_clean.shape}\")\n",
    "\n",
    "# 3. 降维后训练集特征\n",
    "X_train_reduced.to_csv(os.path.join(output_dir, \"X_train_reduced.csv\"), index=False)\n",
    "print(f\"✓ X_train_reduced.csv 已保存 - 形状: {X_train_reduced.shape}\")\n",
    "\n",
    "# 4. 训练集标签\n",
    "y_train.to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False, header=[\"Group\"])\n",
    "print(f\"✓ y_train.csv 已保存 - 长度: {len(y_train)}\")\n",
    "\n",
    "# 5. 原始测试集特征\n",
    "X_test.to_csv(os.path.join(output_dir, \"X_test.csv\"), index=False)\n",
    "print(f\"✓ X_test.csv 已保存 - 形状: {X_test.shape}\")\n",
    "\n",
    "# 6. 清洗后测试集特征\n",
    "X_test_clean.to_csv(os.path.join(output_dir, \"X_test_clean.csv\"), index=False)\n",
    "print(f\"✓ X_test_clean.csv 已保存 - 形状: {X_test_clean.shape}\")\n",
    "\n",
    "# 7. 降维后测试集特征\n",
    "X_test_reduced.to_csv(os.path.join(output_dir, \"X_test_reduced.csv\"), index=False)\n",
    "print(f\"✓ X_test_reduced.csv 已保存 - 形状: {X_test_reduced.shape}\")\n",
    "\n",
    "# 8. 测试集标签\n",
    "y_test.to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False, header=[\"Group\"])\n",
    "print(f\"✓ y_test.csv 已保存 - 长度: {len(y_test)}\")\n",
    "\n",
    "# 额外导出ID信息用于追踪\n",
    "print(\"\\n=== 导出ID信息 ===\")\n",
    "\n",
    "# 9. 训练集ID\n",
    "train_ids.to_csv(os.path.join(output_dir, \"train_ids.csv\"), index=False, header=[\"ID\"])\n",
    "print(f\"✓ train_ids.csv 已保存 - 长度: {len(train_ids)}\")\n",
    "\n",
    "# 10. 测试集ID\n",
    "test_ids.to_csv(os.path.join(output_dir, \"test_ids.csv\"), index=False, header=[\"ID\"])\n",
    "print(f\"✓ test_ids.csv 已保存 - 长度: {len(test_ids)}\")\n",
    "\n",
    "print(f\"\\n所有数据文件已成功导出到: {os.path.abspath(output_dir)}\")\n",
    "print(\"\\n文件列表:\")\n",
    "for file in sorted(os.listdir(output_dir)):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"  {file} ({file_size:.1f} KB)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
